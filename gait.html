<!DOCTYPE html>
<!-- Powered by <a href="https://www.w3schools.com/w3css/default.asp" target="_blank">w3.css</a> -->
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><title>Trung Thanh Ngo | Motion & Structure</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="./Style/import.css">
<link rel="icon" href="Style/tnt2.png" type="image/png" sizes="16x16">

</head>

<body >
	<header class="HeaderBGstyle">	
		<div class="w3-container w3-content" style="max-width:1400px;">		
		 <div class="menu w3-right mainMenuStyle">	<!-- MENU START --> 
		  <a href="index.html" class="w3-bar-item  ">Home</a>		 
		  <a href="profile.html" class="w3-bar-item ">Profile</a>
		  <a href="research.html" class="w3-bar-item  ">Research</a>
		  <a href="publication.html" class="w3-bar-item ">Publication</a>
		  <a href="links.html" class="w3-bar-item ">Links</a>		 
		</div> 		<!-- MENU END --> 

		<div class="w3-container RSHeader">Video and Sensor-based Gait Recognition</div> 
		<div class="menu lowerMenuStyle w3-right">		  
		  <a href="cammotion.html">Camera Motion</a>		 
		  <a href="gait.html"  class="active">Gait Recognition</a>
		  <a href="shape_reflectance.html">Shape & Reflectance</a>
		  <a href="ransac.html">Scale-adaptive</a>
		  <a href="facedettherm.html">Face Detection</a>		 
		</div> 		
		</div>
	</header>

<!-- Page Container -->
<div class="w3-container w3-content w3-margin-top" style="max-width:1400px; min-width:500px;">

<!-- The Grid -->
<div class="w3-row-padding"  >

	
	<div class="BulletHeaderL1">Performance Evaluation of a Large-scale Vision-based Gait Database with Real-life Carried Object</div>
	<div class= "MarginLarge w3-container">
		<div class="w3-third">		
			<img src="Image/gait_bag_dataset.png" width="100%">
		</div>
		
		<div class="w3-twothird RSItemTextbox">
			
			<p>We present a super large dataset of <font color="red">62,528 participants </font>and peformance evaluations for gait recognition against various practical conditions and carrying status recognition. </p>
			
			<ul class="ulArticle MarginSmall">
				<li class="liArticle"> Md. Zasim Uddin, Trung Thanh Ngo, Yasushi Makihara, Noriko Takemura, Xiang Li, Daigo Muramatsu, Yasushi Yagi, 
				<br><span class="pubHL">The OU-ISIR Large Population Gait Database with real-life carried object and its performance evaluation</span>,
				<br>IPSJ Transactions on Computer Vision and Applications, 10: 5 (2018) <a href="https://doi.org/10.1186/s41074-018-0041-z"><big class="PubclassLink"></big></a>, <a href="http://www.am.sanken.osaka-u.ac.jp/BiometricDB/GaitLPBag.html" class="DatasetLink"></a>
				</li>
			</ul>			
		</div>
		
	</div>
	<hr>
	
	<div class="BulletHeaderL1">Sensor-based Gait Recognition with Dynamic Time Warping</div>
	<div class= "MarginLarge w3-container">	
		<div class="w3-third">
			<img src="Image/gaitDTW.png" width="100%">
		</div>
		
		<div class="w3-twothird RSItemTextbox">
			
			<p> We propose a method for constructing gait patterns (with Self-DTW) and a method to improve the pattern quality by eliminating the ambiguity of temporal distortion.</p>
			
			<ul class="ulArticle MarginSmall">
				<li class="liArticle">Trung Thanh Ngo, Yasushi Makihara, Hajime Nagahara, Ryusuke Sagawa, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">Phase Registration in a Gallery Improving Gait Authentication</span>, 
				<br>Proc. of the International Joint Conference on Biometrics (IJCB2011), Washington D.C., USA, Oct. 2011. </li>
				
				<li class="liArticle">Yasushi Makihara, Md.Rasyid Aqmar, Trung Thanh Ngo, Hajime Nagahara, Ryusuke Sagawa, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">Phase Estimation of a Single Quasi-periodic Signal</span>, 
				<br>IEEE Transactions on Signal Processing 62(8): 2066-2079 (2014) <a href="https://doi.org/10.1109/TSP.2014.2306174"><big class="PubclassLink"></big></a>, <a href="http://www.am.sanken.osaka-u.ac.jp/code/SelfDTW.html" class="CodeLink"></a>
				</li> 
					
				<li class="liArticle">Yasushi Makihara, Trung Thanh Ngo, Hajime Nagahara, Ryusuke Sagawa, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">Phase Registration of a Single Quasi-Periodic Signal Using Self Dynamic Time Warping</span>, 
				<br>In Proc. of the 10th Asian Conf. on Computer Vision, pp.1965-1975, Queenstown, New Zealand, Nov., 2010.</li>
			</ul>
		</div>		
	</div>
	<hr>
	
	<div class="BulletHeaderL1">Performance Evaluation of a Large-scale Sensor-based Gait Database </div>
	<div class= "MarginLarge w3-container">
		<div class="w3-third">
			<img src="Image/inertialdataset.png" width="100%">
		</div>
		<div class="w3-twothird RSItemTextbox">
			
			<p>We present the world largest inertial dataset of 744 participants and its performance evaluations against variety of factors, such as age, gender, sensor, etc.</p>
			
			<ul class="ulArticle MarginSmall">
				<li class="liArticle">Trung Thanh Ngo, Yasushi Makihara, Hajime Nagahara, Ryusuke Sagawa, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">Performance Evaluation of Gait Recognition using the Largest Inertial Sensor-based Gait Database</span>, 
				<br>IAPR/IEEE International Conference on Biometrics (ICB2012), New Delhi, India, Apr.2012.</li>
				<li class="liArticle">Trung Thanh Ngo, Yasushi Makihara, Hajime Nagahara, Ryusuke Sagawa, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">The Largest Inertial Sensor-based Gait Database and Performance Evaluation of Gait-based Personal Authentication</span>, 
				<br>Pattern Recognition 47 (2014), pp. 228-237 <a href="https://doi.org/10.1016/j.patcog.2013.06.028"><big class="PubclassLink"></big></a>
				</li> 
			</ul>
		</div>
	</div>
	<hr>

	<div class="BulletHeaderL1">Sensor Orientation-Compensative Signal Regisgration</div>
	<div class= "MarginLarge w3-container">
		<div class="w3-third">
			<img src="Image/sensororientation.png" width="100%">
		</div>
		<div class="w3-twothird RSItemTextbox">
			
			<p>An actual gait signal is always subject to change dueto many factors including variation of sensor attachment. In this research, we tackle to the practical sensor-orientation inconsistency, for which signal sequences are captured at different sensor orientations. We present a method for registering gait signals of rotated sensors and its application in gait recognition.</p>
			
			<ul class="ulArticle MarginSmall">
				<li class="liArticle">Trung Thanh Ngo, Yasushi Makihara, Hajime Nagahara, Ryusuke Sagawa, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">Orientation-Compensative Signal Registration for Owner Authentication using an Accelerometer</span>, 
				<br>IEICE Transactions 97-D(3): 541-553 (2014) <a href="https://doi.org/10.1587/transinf.E97.D.541"><big class="PubclassLink"></big></a>
				</li> 				
			</ul>
		</div>
	</div>
	
	<hr>
	
	<div class="BulletHeaderL1">Gait Recognition with Wearable Camera</div>
	<div class= "MarginLarge w3-container">
		<div class="w3-third">
			<img src="Image/wearablecam.png" width="100%">
		</div>
		<div class="w3-twothird RSItemTextbox">
			
			<p>Inspired by sensor-based gait recognition, we present a solution for gait recognition with wearable camera. We employ a camera motion estimation method and use the motion signal to recognize the person who is wearing the camera.</p>
			
			<ul class="ulArticle MarginSmall">
				<li class="liArticle">Kohei Shiraga, Trung Thanh Ngo, Ikuhisa Mitsugami, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">Gait-based Person Authentication by Wearable Cameras</span>, 
				<br>The ninth International Conference on Networked Sensing Systems (INSS2012), June 2012.</li>
			</ul>
		</div>
	</div>
	<hr>
	
	<div class="BulletHeaderL1">Sensor-based Gait Activity Recognition</div>
	<div class= "MarginLarge w3-container">
		<div class="w3-third">
			<img src="Image/similarActivity.png" width="100%">
		</div>
		<div class="w3-twothird RSItemTextbox">
			
			<p>The research tackles a challenging problem of inertial sensor-based recognition for similar gait action classes (such as walking on
flat ground, up/down stairs, and up/down a slope). We solve three drawbacks of existing methods in the case of gait actions: the action signal segmentation, the sensor orientation inconsistency, and the recognition of similar action classes. </p>
			
			<ul class="ulArticle MarginSmall">
				<li class="liArticle">Trung Thanh Ngo, Yasushi Makihara, Hajime Nagahara, Ryusuke Sagawa, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">Inertial-sensor-based Walking Action Recognition using Robust Step Detection and Inter-class Relationships,</span> 
				<br>Proc. 21th Int. Conf. on Pattern Recognition, Tsukuba, Japan, Nov. 2012. </li>
				<li class="liArticle">Trung Thanh Ngo, Yasushi Makihara, Hajime Nagahara, Yasuhiro Mukaigawa, Yasushi Yagi, 
				<br><span class="pubHL">Similar gait action recognition using an inertial sensor</span>, 
				<br>Pattern Recognition 48(4): 1289-1301 (2015) <a href="https://doi.org/10.1016/j.patcog.2014.10.012"><big class="PubclassLink"></big></a>, <a href="http://www.am.sanken.osaka-u.ac.jp/BiometricDB/SimilarActionsInertialDB.html" class="DatasetLink"></a>
				</li> 
			</ul>
		</div>
	</div>
	<hr>
	
	<div class="BulletHeaderL1">Multimodal Score-level Fusion for Gait</div>
	<div class= "MarginLarge w3-container">
		
		<div class="w3-twothird RSItemTextbox">
						
			<ul class="ulArticle MarginSmall">
				<li class="liArticle">Yasushi Makihara, Daigo Muramatsu, Haruyuki Iwama, Trung Thanh Ngo, Yasushi Yagi, Md. Altab Hossain, 
				<br><span class="pubHL">Score-level fusion by generalized Delaunay triangulation</span>,
				<br>IEEE/IAPR International Joint Conference on Biometrics (IJCB), Sept. 2014</li>
			</ul>
		</div>
	</div>
	
	<hr>
	<br>
	<!-- GO BACK TO RESEARCH -->
	<div class="w3-container">		
		<a href="research.html" class="w3-btn w3-amber w3-large w3-round-xxlarge w3-right">Go back to Research</a>
		<a href="#top" class="myBtn" title="Go to top">Top</a>	
	</div>
	
</div>
</div>

<footer  class="myrelativefooter">
<p>&copy; 2018 Trung Thanh Ngo<p>
</footer>


</body></html>